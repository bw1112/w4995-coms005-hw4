{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET = \"./data/reddit_200k_train_utf-8.csv\"\n",
    "TEST_SET = \"./data/reddit_200k_test_utf-8.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Task 1 Bag of Words and simple Features\n",
    "### 1.1 Create a baseline model using a bag-of-words approach and a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = pd.read_csv(TRAIN_SET, encoding='utf-8')\n",
    "corpus_test = pd.read_csv(TEST_SET, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training dataset is (167529, 8)\n",
      "The size of testing dataset is (55843, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of training dataset is {}\".format(corpus_train.shape))\n",
    "print(\"The size of testing dataset is {}\".format(corpus_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, only use the reddit_200k training and test set, and only use the “body” and “removed” columns. <BR>\n",
    "Pick an appropriate evaluation metric for imbalanced binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = corpus_train[['body', 'REMOVED']]\n",
    "test_set = corpus_test[['body', 'REMOVED']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training dataset is (167529, 2)\n",
      "The size of testing dataset is (55843, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of training dataset is {}\".format(train_set.shape))\n",
    "print(\"The size of testing dataset is {}\".format(test_set.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train has the type of <class 'pandas.core.series.Series'>, and has the size of (167529,)\n",
      "y_train has the type of <class 'pandas.core.series.Series'>, and has the size of (167529,)\n",
      "X_test has the type of <class 'pandas.core.series.Series'>, and has the size of (55843,)\n",
      "y_test has the type of <class 'pandas.core.series.Series'>, and has the size of (55843,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = train_set['body'], train_set['REMOVED']\n",
    "print(f\"X_train has the type of {type(X_train)}, and has the size of {X_train.shape}\")\n",
    "print(f\"y_train has the type of {type(y_train)}, and has the size of {y_train.shape}\")\n",
    "\n",
    "X_test, y_test = test_set['body'], test_set['REMOVED']\n",
    "print(f\"X_test has the type of {type(X_test)}, and has the size of {X_test.shape}\")\n",
    "print(f\"y_test has the type of {type(y_test)}, and has the size of {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Builded vocabulary size: 115231\n"
     ]
    }
   ],
   "source": [
    "# tokenization and building a vocabulary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "print(f\"Builded vocabulary size: {len(vect.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words: <167529x115231 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 5025953 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "X_train_baseline = vect.transform(X_train)\n",
    "print(\"bag_of_words: {}\".format(repr(X_train_baseline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train_baseline, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.71\n"
     ]
    }
   ],
   "source": [
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "\n",
    "X_test_baseline = vect.transform(X_test)\n",
    "print(\"Test score: {:.2f}\".format(grid.score(X_test_baseline, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Try using n-grams, characters, tf-idf rescaling and possibly other ways to tune the BoW model. Be aware that you might need to adjust the (regularization of the) linear model for different feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "              'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "              'tfidfvectorizer__min_df': [1, 2, 3, 4, 5]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"Best cross-validation score: {:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "print(f\"SpaCy version: {spacy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "en_nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(regexp.findall(string))\n",
    "\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document)\n",
    "    return [token.lemma_ for token in doc_spacy]\n",
    "\n",
    "\n",
    "lemma_vect = TfidfVectorizer(tokenizer=custom_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Explore other features you can derive from the text, such as html, length, punctuation, capitalization or other features you deem important from exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Word Vectors\n",
    "\n",
    "### Use a pretrained word-embedding (word2vec, glove or fasttext) instead of the bag-of-words model. Does this improve classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w4995-coms005",
   "language": "python",
   "name": "w4995-coms005"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
